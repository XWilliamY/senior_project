{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Poses\n",
    "\n",
    "Given poses from OpenPose, preprocess them for the model to train on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import medfilt#2d\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Joint Relationships \n",
    "\n",
    "And other functions to help us draw our poses later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_to_limb_heatmap_relationship = [\n",
    "    [1,8],   [1,2],   [1,5],   [2,3],   [3,4],\n",
    "    [5,6],   [6,7],   [8,9],   [9,10],  [10,11],\n",
    "    [8,12],  [12,13], [13,14], [1,0],   [0,15],\n",
    "    [15,17], [0,16],  [16,18]]\n",
    "\n",
    "colors = [\n",
    "    [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0],\n",
    "    [85, 255, 0], [0, 255, 0], [0, 255, 85], [0, 255, 170], [0, 255, 255],\n",
    "    [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], [170, 0, 255],\n",
    "    [255, 0, 255], [255, 0, 170], [255, 0, 85], [255, 0, 0], [255,0,0],\n",
    "    [255,0,0], [255,0,0], [255,0,0], [255,0,0]]\n",
    "\n",
    "\n",
    "def imshow(image):\n",
    "    plt.imshow(image[:,:,[2,1,0]].astype(np.uint8))\n",
    "    plt.show()\n",
    "\n",
    "def add_pose_to_canvas(person_id, coors, canvas, limb_thickness=4):\n",
    "    limb_type = 0\n",
    "    labeled = False\n",
    "    for joint_relation in  joint_to_limb_heatmap_relationship: # [1, 8] for example\n",
    "        joint_coords = coors[joint_relation] # use it as indices \n",
    "        if 0 in joint_coords: # if a keypoint has 0\n",
    "            # ignore keypoints that are not predicted\n",
    "            limb_type += 1\n",
    "            continue\n",
    "        for joint in joint_coords:  # Draw circles at every joint\n",
    "            if not labeled:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cv2.putText(canvas, person_id,\n",
    "                            tuple(joint[0:2].astype(int)), font,\n",
    "                            2, (0,0,0), thickness=5)\n",
    "                labeled = True\n",
    "            cv2.circle(canvas, tuple(joint[0:2].astype(\n",
    "                        int)), 4, (0,0,0), thickness=-1)\n",
    "        coords_center = tuple(\n",
    "                    np.round(np.mean(joint_coords, 0)).astype(int))\n",
    "        limb_dir = joint_coords[0, :] - joint_coords[1, :]\n",
    "        limb_length = np.linalg.norm(limb_dir)\n",
    "        angle = math.degrees(math.atan2(limb_dir[1], limb_dir[0]))\n",
    "        polygon = cv2.ellipse2Poly(\n",
    "                    coords_center, (int(limb_length / 2), limb_thickness), int(angle), 0, 360, 1)\n",
    "        cv2.fillConvexPoly(canvas, polygon, colors[limb_type])\n",
    "        limb_type += 1\n",
    "    return canvas\n",
    "\n",
    "    \n",
    "def draw_pose_figure(person_id, coors, height=1500, width=1500, limb_thickness=4):\n",
    "    canvas = np.ones([height,width,3])*255\n",
    "    canvas = canvas.astype(np.uint8)\n",
    "    limb_type = 0\n",
    "    return add_pose_to_canvas(person_id, coors, canvas, limb_thickness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the relevant input and output directories\n",
    "\n",
    "Input dir should be directory of json files containing pose info\n",
    "\n",
    "Output_dir will be directory we save the final json to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/Users/will.i.liam/Desktop/final_project/openpose/output/'\n",
    "output_dir = '/Users/will.i.liam/Desktop/final_project/phoan/images/'\n",
    "json_files = [pos_json for pos_json in os.listdir(input_dir) if \n",
    "              pos_json.endswith('.json')]\n",
    "\n",
    "json_files = sorted(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 376"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Either allow the user to specify which pose to pick per frame, or use distance algorithm to determine the next most similar pose to the current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(key_a, key_b, conf_a, conf_b):\n",
    "    confs = conf_a * conf_b > 0\n",
    "    confs = confs.reshape(-1, 1)\n",
    "    \n",
    "    return np.sum(((key_a - key_b)**2)*(confs)) / np.sum(confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalized the landmark coordinates within each frame, \n",
    "# by subtracting a reference landmark (the neck) \n",
    "# and dividing by a reference distance (the trunk length)\n",
    "\n",
    "def normalize_in_frame(frame):\n",
    "    \"\"\"\n",
    "    expects frame in the order of openpose outputs\n",
    "    [21, 2] where 2 corresponds to x- y- coordinates\n",
    "    \n",
    "    reduces the pixel coordinate size of each coordinate\n",
    "    \"\"\"\n",
    "    neck = frame[1]\n",
    "    subtracted = frame - neck\n",
    "    trunk_length = np.sqrt(np.sum( (subtracted[8] - subtracted[1]) ** 2))\n",
    "    divided = frame / trunk_length\n",
    "    return divided\n",
    "\n",
    "def adjust_to_canvas(poses):\n",
    "    poses = poses * 125\n",
    "    return poses, np.ones([np.int(poses[8][1] * 2),\n",
    "                    np.int(poses[8][0] * 2), 3]) * 255\n",
    "\n",
    "frame = 2000\n",
    "data = json.load(open(input_dir + json_files[frame]))\n",
    "person_1 = 0\n",
    "person_id_1 = str(person_1) + \", \" + str(data['people'][person_1]['person_id'])\n",
    "a_keys_and_confs = np.array(data['people'][person_1]['pose_keypoints_2d']).reshape(-1, 3)\n",
    "# normalized\n",
    "divided = normalize_in_frame(a_keys_and_confs[:, :2])\n",
    "adjusted_poses = divided * 125\n",
    "canvas = draw_pose_figure(person_id_1, \n",
    "                          adjusted_poses)\n",
    "results = np.hstack([divided, a_keys_and_confs[:, :2]])\n",
    "imshow(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take in frames 370 - 670 of p_hoan: at 30 fps, 300 frames adds up to around 10 seconds of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.arange(370, 670)\n",
    "data = []\n",
    "\n",
    "for frame in frames:\n",
    "    poses = json.load(open(input_dir + json_files[frame]))\n",
    "    data.append(poses['people'][0])\n",
    "\n",
    "keys_and_confs_per_frame = []\n",
    "for datum in data:\n",
    "    keys_and_confs_per_frame.append(np.array(datum['pose_keypoints_2d']).reshape(-1, 3))\n",
    "\n",
    "canvases = []\n",
    "for keys_and_confs in keys_and_confs_per_frame:\n",
    "    person_id = str(0) + \", \" + str([0])\n",
    "    canvases.append(draw_pose_figure(person_id,\n",
    "                                    keys_and_confs[:, :2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'longer_pre_smoothing.mp4'\n",
    "height, width, layers = canvases[0].shape\n",
    "size = (width,height)\n",
    "\n",
    "fourcc_format = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_filename, fourcc_format, 30, size)\n",
    " \n",
    "for i in range(len(canvases)):\n",
    "    out.write(canvases[i])\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 21, 3)\n"
     ]
    }
   ],
   "source": [
    "# keys_and_confs_per_frame = []\n",
    "# an array of multiple 21 x 3\n",
    "np_keys_and_confs_per_frame = np.array(keys_and_confs_per_frame)\n",
    "smoothed = np.copy(np_keys_and_confs_per_frame)\n",
    "print(smoothed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_no_interp = np.copy(np_keys_and_confs_per_frame)\n",
    "window_length, polyorder = 13, 2\n",
    "\n",
    "# kernel = cv2.getGaussianKernel(ksize=(1,1),sigma=2)\n",
    "# kernel = np.transpose(kernel)\n",
    "# oneD = cv2.filter2D(imgIn, -1, kernel)\n",
    "\n",
    "# or use GaussianBlur\n",
    "# cv2.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]])\n",
    "# cv2.GaussianBlur(src, ksize=(1,1))\n",
    "\n",
    "# we need our dataset in the order of (frame, body part, x- y-coordinates)\n",
    "# iterate through all frames, isolate body part and x- y- coordinate\n",
    "for i in range(21): # evaluate per body part\n",
    "    # x or y coordinate\n",
    "    for j in range(2):\n",
    "        a_slice = np_keys_and_confs_per_frame[:, i, j]\n",
    "        orig_shape = a_slice.shape\n",
    "        a_slice = a_slice.reshape(-1)\n",
    "        # smoothed_slice = gaussian_filter1d(a_slice, 100)\n",
    "        med_filtered_slice = medfilt(a_slice)\n",
    "        # smoothed_slice = signal.savgol_filter(med_filtered_slice, window_length, polyorder)\n",
    "        smoothed_slice = cv2.GaussianBlur(med_filtered_slice, (7, 7), 100)\n",
    "        smoothed_no_interp[:, i, j] = smoothed_slice.reshape(orig_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_no_interp_canvases = []\n",
    "for smoothed_keys_and_confs in smoothed_no_interp:\n",
    "    person_id = str(0) + \", \" + str([0])\n",
    "    smoothed_no_interp_canvases.append(draw_pose_figure(person_id,\n",
    "                                                        smoothed_keys_and_confs[:, :2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'longer_smoothing_no_interp.mp4'\n",
    "height, width, layers = smoothed_no_interp_canvases[0].shape\n",
    "size = (width,height)\n",
    "\n",
    "fourcc_format = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_filename, fourcc_format, 30, size)\n",
    " \n",
    "for i in range(len(smoothed_no_interp_canvases)):\n",
    "    out.write(smoothed_no_interp_canvases[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that their example is just x position and velocity\n",
    "but we'll need x position y position and x velocity y velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[786.447 786.452 789.167 789.287 789.296   0.    795.251 798.09  806.872\n",
      " 809.867 812.833 815.825 824.66  833.272 836.353 839.401 851.057 857.055\n",
      " 862.935 877.518 883.611 898.142 907.043 915.888 927.642 933.566 948.147\n",
      " 954.081 962.932 974.76 ]\n",
      "None\n",
      "[786.4519800796812, 433.0300199203187]\n",
      "[789.1562259410919, 433.10370642361386]\n",
      "[789.2904079361014, 433.16202008834125]\n",
      "[789.2996213950498, 433.17810350181395]\n",
      "[792.2887311893062, 430.1543797570764]\n",
      "[795.2505361332397, 427.2132772143813]\n",
      "[798.0886964696806, 424.3589319410451]\n",
      "[806.7600288544443, 418.4937662053538]\n",
      "[809.9028931582784, 415.4778822294915]\n",
      "[812.864668314688, 412.69449536956967]\n",
      "[815.9093996101677, 412.4900492983572]\n",
      "[820.5354515932045, 407.7865097460192]\n",
      "[836.80952845405, 416.55637775986224]\n",
      "[835.1461931012581, 411.797695545889]\n",
      "[839.1091284483192, 412.56057353737066]\n",
      "[872.1458221643046, 399.28401999313564]\n",
      "[860.2079669784197, 405.5822792015752]\n",
      "[863.898782480542, 403.34078740516634]\n",
      "[872.7428389785754, 396.1022685032807]\n",
      "[881.632586981933, 388.9141523241322]\n",
      "[894.5268178366995, 381.8454662250479]\n",
      "[905.3343302415781, 370.0802809982707]\n",
      "[915.0788672167222, 361.47351320751966]\n",
      "[925.9137427484211, 352.6580439385872]\n",
      "[934.3535956543266, 349.73333928449114]\n",
      "[945.8275683907214, 345.6987223451672]\n",
      "[954.7435425462463, 346.9201250154521]\n",
      "[963.5476265590459, 349.8231235700861]\n",
      "[973.840335278089, 355.0643181034738]\n"
     ]
    }
   ],
   "source": [
    "copy = np.copy(np_keys_and_confs_per_frame)\n",
    "original = copy[5, 3, 0]\n",
    "np_keys_and_confs_per_frame[5, 3, 0] = 0\n",
    "\n",
    "missing_coord = 0\n",
    "frames = np.where(np_keys_and_confs_per_frame[:, 3, 0] != 0)[0]\n",
    "actual_coords = [0]\n",
    "print(np_keys_and_confs_per_frame[:, 3, 0])\n",
    "interpolated_x = np.interp(np.arange(30), frames, np_keys_and_confs_per_frame[:, 3, 0][frames])\n",
    "interpolated_y = np.interp(np.arange(30), frames, np_keys_and_confs_per_frame[:, 3, 1][frames])\n",
    "# apply Kalman\n",
    "for i in range(30):\n",
    "    x = interpolated_x[i]\n",
    "    y = interpolated_y[i]\n",
    "    print(KF(x, y, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed = np.copy(np_keys_and_confs_per_frame)\n",
    "\n",
    "for i in range(21): # evaluate per body part\n",
    "    # x or y coordinate\n",
    "    for j in range(2):\n",
    "        a_slice = np_keys_and_confs_per_frame[:, i, j]\n",
    "        orig_shape = a_slice.shape\n",
    "        a_slice = a_slice.reshape(-1)\n",
    "        \n",
    "        frames = np.where(a_slice != 0)[0]\n",
    "        interpolated = np.interp(np.arange(orig_shape[0]), frames, a_slice[frames])\n",
    "        # smoothed_slice = gaussian_filter1d(a_slice, 100)\n",
    "        med_filtered_slice = medfilt(interpolated)\n",
    "        # smoothed_slice = signal.savgol_filter(med_filtered_slice, window_length, polyorder)\n",
    "        smoothed_slice = cv2.GaussianBlur(med_filtered_slice, (7, 7), 100)\n",
    "        smoothed[:, i, j] = smoothed_slice.reshape(orig_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_canvases = []\n",
    "for k_c in smoothed:\n",
    "    person_id = str(0) + \", \" + str([0])\n",
    "    interpolated_canvases.append(draw_pose_figure(person_id,\n",
    "                                                  k_c[:, :2]))\n",
    "    \n",
    "output_filename = 'longer_smoothed_interp.mp4'\n",
    "height, width, layers = interpolated_canvases[0].shape\n",
    "size = (width,height)\n",
    "\n",
    "fourcc_format = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_filename, fourcc_format, 30, size)\n",
    " \n",
    "for i in range(len(interpolated_canvases)):\n",
    "    out.write(interpolated_canvases[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
